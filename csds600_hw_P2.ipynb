{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"csds600_hw_P2.ipynb","provenance":[{"file_id":"1mscYLEWt3BzZ_ob9C41690X8i9vgORGA","timestamp":1615271509572}],"collapsed_sections":[],"authorship_tag":"ABX9TyO5BL6dKsWGhz9cSa5ntLTD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"x-uAR_W7b7Yr","executionInfo":{"status":"ok","timestamp":1615354218749,"user_tz":300,"elapsed":204,"user":{"displayName":"Xufei Wang","photoUrl":"","userId":"17277511209364948654"}}},"source":["import numpy as np\r\n","import math"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxzbu8da01YD"},"source":["Build the NN"]},{"cell_type":"code","metadata":{"id":"oDpMxa3-ivkM","executionInfo":{"status":"ok","timestamp":1615354219032,"user_tz":300,"elapsed":483,"user":{"displayName":"Xufei Wang","photoUrl":"","userId":"17277511209364948654"}}},"source":["class ANN(object):\r\n","    def __init__(self, layers = [2 , 3, 2], activations=['sigmoid', 'sigmoid'],weights=[],biases=[]):\r\n","        assert(len(layers) == len(activations)+1)\r\n","        self.layers = layers\r\n","        self.activations = activations\r\n","        self.weights = weights\r\n","        self.biases = biases\r\n","\r\n","    \r\n","    def feedforward(self, x):\r\n","        # return the feedforward value for x, node_vlaue is the value holds on neurons; output_value is the output of neurons\r\n","        a = np.copy(x)\r\n","        node_value = []\r\n","        output_value = [a]\r\n","        for i in range(len(self.weights)):\r\n","            activation_function = self.getActivationFunction(self.activations[i])\r\n","            node_value.append(self.weights[i].dot(a) + self.biases[i])\r\n","            a = activation_function(node_value[-1])\r\n","            output_value.append(a)\r\n","        return (node_value, output_value)\r\n","\r\n","    \r\n","    def backpropagation(self,y, node_value, output_value):\r\n","        dw = []  # dC/dW\r\n","        db = []  # dC/dB\r\n","        deltas = [None] * len(self.weights)  # delta = dC/dZ  known as error for each layer\r\n","        # insert the last layer error\r\n","        deltas[-1] = ((y-output_value[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(node_value[-1]))\r\n","        # Perform BackPropagation\r\n","        for i in reversed(range(len(deltas)-1)):\r\n","            deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(node_value[i]))        \r\n","       \r\n","        batch_size = y.shape[1]\r\n","        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\r\n","        dw = [d.dot(output_value[i].T)/float(batch_size) for i,d in enumerate(deltas)]\r\n","        # return the derivitives respect to weight matrix and biases\r\n","        return dw, db\r\n","\r\n","    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\r\n","# update weights and biases based on the output\r\n","         for e in range(epochs): \r\n","            i=0\r\n","            while(i<len(y)):\r\n","                x_batch = x[i:i+batch_size].reshape(2,1)\r\n","                y_batch = y[i:i+batch_size].reshape(2,1)\r\n","                i = i+batch_size\r\n","\r\n","                node_value, output_value = self.feedforward(x_batch)\r\n","                dw, db = self.backpropagation(y_batch, node_value, output_value)\r\n","                print(\"dw are {}:\".format(dw))\r\n","                print(\"db are {}:\".format(db))\r\n","                self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\r\n","                print(\"weights are {}:\".format(self.weights))\r\n","                self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\r\n","                print(\"biases are {}:\".format(self.biases))\r\n","                print(\"loss = {}\".format(np.linalg.norm(output_value[-1]-y_batch) ))\r\n","    @staticmethod\r\n","    def getActivationFunction(name):\r\n","        if(name == 'sigmoid'):\r\n","            return lambda x : np.exp(x)/(1+np.exp(x))\r\n","        elif(name == 'linear'):\r\n","            return lambda x : x\r\n","        elif(name == 'relu'):\r\n","            def relu(x):\r\n","                y = np.copy(x)\r\n","                y[y<0] = 0\r\n","                return y\r\n","            return relu\r\n","        else:\r\n","            print('Unknown activation function. linear is used')\r\n","            return lambda x: x\r\n","    \r\n","    @staticmethod\r\n","    def getDerivitiveActivationFunction(name):\r\n","        if(name == 'sigmoid'):\r\n","            sig = lambda x : np.exp(x)/(1+np.exp(x))\r\n","            return lambda x :sig(x)*(1-sig(x)) \r\n","        elif(name == 'linear'):\r\n","            return lambda x: 1\r\n","        elif(name == 'relu'):\r\n","            def relu_diff(x):\r\n","                y = np.copy(x)\r\n","                y[y>=0] = 1\r\n","                y[y<0] = 0\r\n","                return y\r\n","            return relu_diff"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j46o3juDUkm4"},"source":["test the ann by puttitng T1 into network"]},{"cell_type":"code","metadata":{"id":"C1qeCfNpdG1X","executionInfo":{"status":"ok","timestamp":1615354219033,"user_tz":300,"elapsed":481,"user":{"displayName":"Xufei Wang","photoUrl":"","userId":"17277511209364948654"}}},"source":["  w=[]\r\n","  b=[]\r\n","  w.append(np.array([[0.1,-0.2],[0,0.2],[0.3,-0.4]]))#w13,w23 w14,w24 w15,w25\r\n","  w.append(np.array([[-0.4,0.1,0.6],[0.2,-0.1,-0.2]]))#w36,w46,w56 ... \r\n","  b.append(np.array([[0.1],[0.2],[0.5]]))# b3 b4 b5 \r\n","  b.append(np.array([[-0.1],[0.6]]))\r\n","  #manully setup the weights and biases \r\n","  #use liner as activation function to verify the homework calculation\r\n","  #sigmoid it the more common option\r\n","  nn = ANN([2, 3, 2],activations=['linear', 'linear'],weights=w,biases=b)\r\n","  T1=np.array([[0.6],[0.1]])\r\n","  y1= np.array([[1],[0]])\r\n","  T2=np.array([[0.2],[0.3]])\r\n","  y2= np.array([[0],[1]])\r\n","  data=np.append(T1,T2).reshape(2,2)\r\n","  label =np.append(y1,y2).reshape(2,2)\r\n","  "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cPrLkLYJ1cyX","executionInfo":{"status":"ok","timestamp":1615354219034,"user_tz":300,"elapsed":478,"user":{"displayName":"Xufei Wang","photoUrl":"","userId":"17277511209364948654"}},"outputId":"d62f643a-9ef0-451c-9ddc-5b0bbbe37448"},"source":["node_value, output_value = nn.feedforward(T1)\r\n","print(\"node_value are {}:\".format(node_value))\r\n","print(\"output_value are {}:\".format(output_value))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["node_value are [array([[0.14],\n","       [0.22],\n","       [0.64]]), array([[0.25 ],\n","       [0.478]])]:\n","output_value are [array([[0.6],\n","       [0.1]]), array([[0.14],\n","       [0.22],\n","       [0.64]]), array([[0.25 ],\n","       [0.478]])]:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALLrn5NJYqnI","executionInfo":{"status":"ok","timestamp":1615354219036,"user_tz":300,"elapsed":476,"user":{"displayName":"Xufei Wang","photoUrl":"","userId":"17277511209364948654"}},"outputId":"a2ccbdce-3649-42eb-b4b1-1387861426ff"},"source":["nn.train(data, label, epochs=1, batch_size=1, lr = 0.1)   "],"execution_count":5,"outputs":[{"output_type":"stream","text":["dw are [array([[-0.23736, -0.03956],\n","       [ 0.07368,  0.01228],\n","       [ 0.32736,  0.05456]]), array([[ 0.105  ,  0.165  ,  0.48   ],\n","       [-0.06692, -0.10516, -0.30592]])]:\n","db are [array([[-0.3956],\n","       [ 0.1228],\n","       [ 0.5456]]), array([[ 0.75 ],\n","       [-0.478]])]:\n","weights are [array([[ 0.076264, -0.203956],\n","       [ 0.007368,  0.201228],\n","       [ 0.332736, -0.394544]]), array([[-0.3895  ,  0.1165  ,  0.648   ],\n","       [ 0.193308, -0.110516, -0.230592]])]:\n","biases are [array([[0.06044],\n","       [0.21228],\n","       [0.55456]]), array([[-0.025 ],\n","       [ 0.5522]])]:\n","loss = 0.8893728127169168\n","dw are [array([[ 0.04833572,  0.07250358],\n","       [-0.02068842, -0.03103262],\n","       [-0.06965349, -0.10448024]]), array([[-0.00474438, -0.08965523, -0.16442908],\n","       [ 0.00857623,  0.16206627,  0.29723205]])]:\n","db are [array([[ 0.24167859],\n","       [-0.10344208],\n","       [-0.34826746]]), array([[-0.32706324],\n","       [ 0.59121949]])]:\n","weights are [array([[ 0.08109757, -0.19670564],\n","       [ 0.00529916,  0.19812474],\n","       [ 0.32577065, -0.40499202]]), array([[-0.38997444,  0.10753448,  0.63155709],\n","       [ 0.19416562, -0.09430937, -0.2008688 ]])]:\n","biases are [array([[0.08460786],\n","       [0.20193579],\n","       [0.51973325]]), array([[-0.05770632],\n","       [ 0.61132195]])]:\n","loss = 0.6756558604403696\n"],"name":"stdout"}]}]}