{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-quantum",
   "metadata": {},
   "source": [
    "# P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incoming-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libsvm.svmutil import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-optimum",
   "metadata": {},
   "source": [
    "## (1)SVM Accuracy and Cross-Validation for Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norwegian-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "label_train, data_train = svm_read_problem(\n",
    "    'D:/course/csds600_ye/CSDS600_HW4_DogsVsCats/DogsVsCats/DogsVsCats/DogsVsCats.train')\n",
    "label_test, data_test = svm_read_problem(\n",
    "    'D:/course/csds600_ye/CSDS600_HW4_DogsVsCats/DogsVsCats/DogsVsCats/DogsVsCats.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handed-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = np.asarray(label_train)\n",
    "data_train = np.asarray(data_train)\n",
    "label_test = np.asarray(label_test)\n",
    "data_test = np.asarray(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hidden-belle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500,)\n",
      "(12500,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data_train))\n",
    "print(np.shape(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handmade-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold, use StratifiedKFold to make suree the sub_sets of data follow same distribution\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-ballot",
   "metadata": {},
   "source": [
    "### linear kernal svm 10-fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "senior-rental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 61.52% (769/1250) (classification)\n",
      "Accuracy = 58% (725/1250) (classification)\n",
      "Accuracy = 59.84% (748/1250) (classification)\n",
      "Accuracy = 59.68% (746/1250) (classification)\n",
      "Accuracy = 59.44% (743/1250) (classification)\n",
      "Accuracy = 58.32% (729/1250) (classification)\n",
      "Accuracy = 61.12% (764/1250) (classification)\n",
      "Accuracy = 60.72% (759/1250) (classification)\n",
      "Accuracy = 60.24% (753/1250) (classification)\n",
      "Accuracy = 59.76% (747/1250) (classification)\n",
      "validation accuracy : 59.864\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for train_index, val_index in skf.split(data_train,label_train):\n",
    "    label_train_loop, data_train_loop = label_train[train_index].tolist(), data_train[train_index].tolist()\n",
    "    label_test_loop, data_test_loop = label_train[val_index].tolist(), data_train[val_index].tolist()\n",
    "    linear_kernal = svm_train(label_train_loop, data_train_loop, '-s 0 -t 0')\n",
    "    #-t 0 linear kernal \n",
    "    label, acc, val = svm_predict(label_test_loop, data_test_loop, linear_kernal)\n",
    "    accuracy += acc[0]\n",
    "accuracy /= 10\n",
    "print(f'validation accuracy : {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-wisconsin",
   "metadata": {},
   "source": [
    "### all data train linear svm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incomplete-sydney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainng...\n",
      "Accuracy = 60.12% (7515/12500) (classification)\n",
      "Testing...\n",
      "Accuracy = 59.2% (7400/12500) (classification)\n"
     ]
    }
   ],
   "source": [
    "linear_kernal = svm_train(label_train, data_train, '-s 0 -t 0')\n",
    "print('Trainng...')\n",
    "label, acc, val = svm_predict(label_train, data_train, linear_kernal)\n",
    "print('Testing...')\n",
    "label, acc, val = svm_predict(label_test, data_test, linear_kernal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-browse",
   "metadata": {},
   "source": [
    "### polynomial kernel  svm 10-fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "contemporary-cloud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 50.08% (626/1250) (classification)\n",
      "Accuracy = 49.92% (624/1250) (classification)\n",
      "Accuracy = 50.08% (626/1250) (classification)\n",
      "Accuracy = 50% (625/1250) (classification)\n",
      "Accuracy = 50% (625/1250) (classification)\n",
      "Accuracy = 50.08% (626/1250) (classification)\n",
      "Accuracy = 50.08% (626/1250) (classification)\n",
      "Accuracy = 50.08% (626/1250) (classification)\n",
      "Accuracy = 50% (625/1250) (classification)\n",
      "Accuracy = 49.92% (624/1250) (classification)\n",
      "validation accuracy: 50.024\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for train_index, val_index in skf.split(data_train,label_train):\n",
    "    label_train_loop, data_train_loop = label_train[train_index].tolist(), data_train[train_index].tolist()\n",
    "    label_test_loop, data_test_loop = label_train[val_index].tolist(), data_train[val_index].tolist()\n",
    "    linear_kernal = svm_train(label_train_loop, data_train_loop, '-s 0 -t 1 -d 5')\n",
    "    #poly kernal with d=5\n",
    "    label, acc, val = svm_predict(label_test_loop, data_test_loop, linear_kernal)\n",
    "    accuracy += acc[0]\n",
    "accuracy /= 10\n",
    "print(f'validation accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-murder",
   "metadata": {},
   "source": [
    "###  all data train polynomial kernel svm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confidential-quest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Accuracy = 50.024% (6253/12500) (classification)\n",
      "Testing...\n",
      "Accuracy = 50.048% (6256/12500) (classification)\n"
     ]
    }
   ],
   "source": [
    "poly_kernal = svm_train(label_train, data_train, '-s 0 -t 1 -d 5')\n",
    "print('Training...')\n",
    "label, acc, val = svm_predict(label_train, data_train, poly_kernal)\n",
    "print('Testing...')\n",
    "label, acc, val = svm_predict(label_test, data_test, poly_kernal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-library",
   "metadata": {},
   "source": [
    "### discussion\n",
    "From the results, we could observe that for the problem, liner kernal is better than polynimial kernal with dimension 5. Poly kernal basiclly is random guessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-furniture",
   "metadata": {},
   "source": [
    "## (2)Boosting SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "noble-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liv_svm2sk_data(data):\n",
    "    data_sk = []\n",
    "    for i in data:\n",
    "        temp = []\n",
    "        for j in range(1, 65):\n",
    "            temp.append(i[j])\n",
    "        data_sk.append(temp)\n",
    "    data_sk = np.asarray(data_sk)     \n",
    "    return data_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "scientific-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_sk=liv_svm2sk_data(data_train)\n",
    "data_test_sk=liv_svm2sk_data(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-roulette",
   "metadata": {},
   "source": [
    "## AdaBoost Linear SVM\n",
    "since linear SVM has better performence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "plastic-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59184"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_l = SVC(kernel='linear')\n",
    "svm_l.fit(data_train_sk, label_train)\n",
    "svm_l.score(data_test_sk, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floating-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=svm_l, n_estimators=10, algorithm='SAMME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "synthetic-mandate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME', base_estimator=SVC(kernel='linear'),\n",
       "                   n_estimators=10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_train_sk, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "informal-chambers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55072"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(data_test_sk, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-divorce",
   "metadata": {},
   "source": [
    "### discussion\n",
    "The result is very interesting, the adaboost did't improve the accuracy, it turn worse actually. There are some possiable reasons at that point\n",
    "1)the K=10 is too small, the adaboost svm did't converage\n",
    "2)using adaboost in SVM can't improve the accuracy for reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-seminar",
   "metadata": {},
   "source": [
    "## (3)Increasing the number of boosting iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hungarian-bundle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55072"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=svm_l, n_estimators=20, algorithm='SAMME')\n",
    "clf.fit(data_train_sk, label_train)\n",
    "clf.score(data_test_sk, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-mailman",
   "metadata": {},
   "source": [
    "Increase the K did't change the accuracy, then we use the defult clssifier, the decision tree of adaboostclassifier function test it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cooperative-response",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64424"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=20)\n",
    "clf.fit(data_train_sk, label_train)\n",
    "clf.score(data_test_sk, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "respected-subcommittee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67184"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=200)\n",
    "clf.fit(data_train_sk, label_train)\n",
    "clf.score(data_test_sk, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-moisture",
   "metadata": {},
   "source": [
    "The result is way mush better! And the accuracy of decision tree is increasing when we using a bigger K(20->200,0.64->0.67). We did't even tunning the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-pantyhose",
   "metadata": {},
   "source": [
    "### discussion\n",
    "From above, we find that increse iteration did't help improve svm's accuracy in adaboost. That basiclly means the situation in (2) is not caused by converage problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-eligibility",
   "metadata": {},
   "source": [
    "## (4) Conclusion\n",
    "From the experments, we have:\n",
    "1) for the dogs vs cats clssify probelm, liner kernal is better than d-5 polynomial kernal SVM\n",
    "2) its a bad idea using adaboost to optimize the SVM classifier \n",
    "3) weak classifier(decision tree) has better performence in adaboot\n",
    "\n",
    "From above, we got our conclution: Using SVMs as base classifiers for Adaboost is a bad idea. \n",
    "\n",
    "Adaboost (and similar ensemble methods) were conceived using decision trees as base classifiers (more specifically, decision stumps, i.e. DTs with a depth of only 1);  DTs are suitable for such ensembling because they are essentially unstable classifiers, which is not the case with SVMs, hence the latter are not expected to offer much when used as base classifiers. \n",
    "\n",
    "On top of this, SVMs are computationally much more expensive than decision trees (let alone decision stumps), which is cause  the long processing times for traing the SVM.  Unless we have a very good reason to stick to SVMs as base classifiers for adaboost(I  doubt if there is one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-gateway",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
